{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###.... This is an integrated snop program that outputs c4s.csv and supply.csv from a set of inputs.\n",
    "###.... More details can be found in README.MD\n",
    "\n",
    "#********************************************************************\n",
    "# Notes:\n",
    "\n",
    "# 1) FourthShift Auto-updates happen everyday at 6:00 PM GMT \n",
    "#_____________________________________________________________________\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import calendar\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from xlrd import open_workbook\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # Disable pesky chained_assignment warnings\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#**************************************************************\n",
    "#                 Flags for manipulation\n",
    "#______________________________________________________________\n",
    "\n",
    "# Done in the first week of every month\n",
    "no_india_orders = False    # If India does'nt have any enough orders (month beginning)\n",
    "\n",
    "remove_outliers = True # Flag for removing margin outliers\n",
    "negative_outlier = -0.25\n",
    "positive_outlier = 2\n",
    "\n",
    "use_local_file = True    # For Krishnan's file\n",
    "\n",
    "local_order_file = r'\\FS_Orders.xls'\n",
    "local_bom_file = r'\\FS_BILL.xlsx'\n",
    "\n",
    "\n",
    "# path to store output\n",
    "localpath = '.'\n",
    "# localpath = r\"C:\\Users\\kashir\\Documents\\SnOP\\00_Model\"\n",
    "\n",
    "#**************************************************************\n",
    "#                 Date conversion functions\n",
    "#______________________________________________________________\n",
    "\n",
    "def get_date(s_year, s_month):\n",
    "    '''Function to get date from year and month\n",
    "       Arg: \n",
    "         (s_year, s_month as <string, int>)\n",
    "       Returns: \n",
    "         date as datetime.date'''\n",
    "    dt = pd.to_datetime(int(s_year)*10000+int(s_month)*100+1, format = \"%Y%m%d\")\n",
    "    return dt.date()\n",
    "\n",
    "def get_last_day(idt):\n",
    "    '''Function to get the last date of a month\n",
    "       Arg: \n",
    "          idt as date\n",
    "       Returns:\n",
    "          last date as datetime.date\n",
    "       '''\n",
    "    dt = pd.to_datetime(idt) + pd.tseries.offsets.MonthEnd(1)\n",
    "    return dt.date()\n",
    "\n",
    "#*********************************************************************\n",
    "#                      Variables and File Paths\n",
    "#_____________________________________________________________________\n",
    "\n",
    "# SnOP Horizon in months\n",
    "snop_horizon = 15\n",
    "\n",
    "# Last day of this month\n",
    "thismonthend = (datetime.datetime.now()+pd.tseries.offsets.MonthEnd(1)).date()\n",
    "\n",
    "# Site ID standardization\n",
    "siteID_dict = {\"IN\": \"ID\", \"BA\": \"IN\", \"J1\": \"HU\", \n",
    "               \"DB\": \"AE\", \"GR\": \"US\", \"HU\": \"HU\", \n",
    "               \"TH\": \"TH\", \"SG\": \"SG\", \"PA\": \"PA\", \n",
    "               \"BR\": \"PA\"}\n",
    "\n",
    "# Basic columns\n",
    "base_cols = ['DataType', 'Site', 'ProductID', 'ProductDesc', \n",
    "             'CustomerNo', 'CustName', 'CustCountry', 'AM',  \n",
    "             'Order', 'By', 'Qty', 'GBPValue']\n",
    "\n",
    "#... File Paths ...\n",
    "#..................\n",
    "\n",
    "# Local files\n",
    "# localpath = r\"C:\\Users\\kashir\\Documents\\SnOP\\00_Model\"\n",
    "\n",
    "# Live files\n",
    "livecognospath = r\"\\\\10.5.20.5\\00_Model\\Cognos\"\n",
    "livefspath = r\"\\\\10.5.20.5\\00_Model\\FS\\FSLive\"\n",
    "\n",
    "# Cognos and FouthShift files\n",
    "if use_local_file is True:\n",
    "    fs_orders_file = localpath + local_order_file\n",
    "    fs_bom_file = localpath + local_bom_file\n",
    "    cis_file = localpath + r'\\CIS.txt'\n",
    "    volume_file = localpath + r'\\Volume.txt'\n",
    "else:\n",
    "    fs_orders_file = livefspath + r'\\FS_Orders.xls'\n",
    "    fs_bom_file = livefspath + r'\\FS_BILL.xlsx'\n",
    "    cis_file = livecognospath + r'\\CIS_a.txt'\n",
    "    volume_file = livecognospath + r'\\Volume_a.txt'\n",
    "\n",
    "# Actuals file (from Darshil)\n",
    "actuals_file = localpath + r'\\Actuals.xlsx'\n",
    "\n",
    "# Brazil file (from Luiz / Diego)\n",
    "brazil_file = localpath + r'\\BRAZIL.xlsx'\n",
    "\n",
    "# F1, F2, F3\n",
    "f1_file = localpath + r'\\F1.xlsx'\n",
    "f2_file = localpath + r'\\F2.xlsx'\n",
    "f3_file = localpath + r'\\F3.xlsx'\n",
    "plan_file = localpath + r'\\Plan.xlsx'\n",
    "\n",
    "# Lookup file\n",
    "lookups_file = localpath + r'\\SnOP Lookups.xlsx'\n",
    "lookups_xls = pd.ExcelFile(lookups_file)\n",
    "\n",
    "#... File-based DataFrames ...\n",
    "#.............................\n",
    "\n",
    "cognos_cols = 'DataType,Combi,Month,Year,Qty_Val'.split(',')\n",
    "\n",
    "df_cis = pd.read_csv(cis_file, sep=None, encoding = \"UTF-16\", \n",
    "                        header=None, engine='python', names = cognos_cols, usecols=[0, 1, 3, 4, 5])\n",
    "df_volume =  pd.read_csv(volume_file, sep=None, encoding = \"UTF-16\", \n",
    "                        header=None, engine='python', names = cognos_cols, usecols=[0, 1, 3, 4, 5])\n",
    "\n",
    "df_orders = pd.read_excel(fs_orders_file, header=0)\n",
    "df_orders.PromDock_InvoiceDate = pd.to_datetime(df_orders.PromDock_InvoiceDate).dt.date\n",
    "\n",
    "df_bom = pd.read_excel(fs_bom_file, header=0)\n",
    "df_bom = df_bom.drop(axis=0, index=0) # drop the first row\n",
    "df_bom = df_bom.drop(columns='IsBR')\n",
    "\n",
    "df_actuals = pd.read_excel(actuals_file, 'Actuals', skiprows=4, header=0)\n",
    "df_actuals.By = pd.to_datetime(df_actuals.By).dt.date\n",
    "\n",
    "df_brazil = pd.read_excel(brazil_file, header=0)\n",
    "df_brazil.By = pd.to_datetime(df_brazil.By).dt.date\n",
    "\n",
    "# F1, F2, F3, ...\n",
    "df_f1 = pd.read_excel(f1_file, 'F1', skiprows=4, header=0)\n",
    "df_f2 = pd.read_excel(f2_file, 'F2', skiprows=4, header=0)\n",
    "df_f3 = pd.read_excel(f3_file, 'F3', skiprows=4, header=0)\n",
    "df_plan19 = pd.read_excel(plan_file, skiprows=4, header=0)\n",
    "\n",
    "#... Lookups DataFrame ...\n",
    "#.......................\n",
    "df_ryo_pop = pd.read_excel(lookups_xls, 'RYOPOP', header=0)\n",
    "# df_divadj = pd.read_excel(lookups_xls, '2018DivAdjust', header=0)\n",
    "df_am = pd.read_excel(lookups_xls, 'AM_names', header=0).drop_duplicates('AM Code')\n",
    "df_custgrp = pd.read_excel(lookups_xls, 'Customer_names', \n",
    "                           header=0).drop_duplicates('CustName')\n",
    "\n",
    "# df_region = pd.read_excel(lookups_xls, 'CountryRef', header=0).drop_duplicates('Country')\n",
    "df_prodref = pd.read_excel(lookups_xls, 'ProductRef', header=0, skiprows=4)\n",
    "df_prodref = df_prodref.rename(columns = {'S1': 'Seg1', \n",
    "                                            'S2': 'Seg2', 'S3': 'Seg3', 'S4': 'Seg4' })\n",
    "df_gamechangers = pd.read_excel(lookups_xls, 'GameChangers', header=0, skiprows=4, usecols=[1,2])\n",
    "\n",
    "df_exceptions = pd.read_excel(lookups_xls, 'Master', skiprows=4, header=0, usecols = 'Y').drop_duplicates('Logic')\n",
    "df_mfgsitechange = pd.read_excel(lookups_xls, 'MfgSite', header=4)\n",
    "\n",
    "df_machine = pd.read_excel(lookups_xls, sheet_name='MachineRef', header=0, skiprows=4, \n",
    "                           converters={'MachineID': str})\n",
    "df_machinemap = pd.read_excel(lookups_xls, sheet_name='MachineMap', header=0, skiprows=4, \n",
    "                              converters={'MachineID': str}, usecols=[0,1,2,3,4,5])\n",
    "\n",
    "df_sites = pd.read_excel(lookups_xls, sheet_name='Master', header=0, skiprows=4, usecols=\"O:P\").dropna()\n",
    "df_holidays = pd.read_excel(lookups_xls, sheet_name='Master', header=0, skiprows=4, usecols=\"Q:R\")\n",
    "\n",
    "df_periods = pd.read_excel(lookups_xls, sheet_name='Master', header=0, skiprows=4, usecols=\"AA:AC\")\n",
    "df_periods = df_periods.dropna()\n",
    "\n",
    "# Generate EssBy from Year_Period\n",
    "df_periods['EssBy'] = [get_last_day(get_date(x, y)) for x, y in df_periods.Year_Period.str.split('-').tolist()]\n",
    "df_periods.iloc[:, 1:4] = df_periods.iloc[:, 1:4].apply(pd.to_datetime, errors='coerce') # convert to datetime\n",
    "df_periods.iloc[:, 1:4] = df_periods.iloc[:, 1:4].apply(lambda x: x.dt.date) # convert to datetime.date\n",
    "\n",
    "# Get the start date as the minimum of Year_Period\n",
    "start_date = pd.to_datetime(df_periods.Start.min()).date()\n",
    "\n",
    "years = [start_date.year, start_date.year+2] #isin requires array\n",
    "\n",
    "# Generate dictionary for mapping periods to Year_Period based on Essentra Calendar\n",
    "df_essdict = df_periods.set_index('Year_Period')['EssBy'].to_dict()\n",
    "\n",
    "def get_essby(df, start_date=start_date):\n",
    "    '''Args: \n",
    "         df as pd dataframe, <start_date> defaulted from minum of df_periods.Start date\n",
    "       Output:\n",
    "         df with EssBy column'''\n",
    "    \n",
    "    df.By = pd.to_datetime(df.By).dt.date\n",
    "\n",
    "    # Select only from the minimum start period if given\n",
    "    if start_date != None:\n",
    "        df = df.loc[df.By >= start_date, :].reset_index(drop=True)\n",
    "\n",
    "    df['EssBy'] = [k for x in df.By \n",
    "                           for i, j, k in zip(df_periods.Start, df_periods.End, df_periods.EssBy) \n",
    "                           if i <= x <= j]\n",
    "    return df\n",
    "\n",
    "# %%time\n",
    "#*********************************************************************\n",
    "#                            Cognos Cleanup     \n",
    "#_____________________________________________________________________\n",
    "\n",
    "#...   Get the Forecast ...\n",
    "#..........................\n",
    "\n",
    "# Filter Forecast\n",
    "forecastfilter = (((df_cis.DataType == 'Actual / Forecast') | (df_cis.DataType == 'Value GBP')) &\n",
    "                (df_cis.Year.isin(years)) & \\\n",
    "                (df_cis.Month.isin(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])) & \\\n",
    "                (df_cis.Combi != 'TOTAL') & \\\n",
    "                (df_cis.Qty_Val != 0))\n",
    "\n",
    "df0_forecast = df_cis[forecastfilter]\n",
    "\n",
    "# Pivot the values to get Forecast, Month and Year Values\n",
    "df1_forecast = df0_forecast.pivot_table(index = ['Combi', 'Month', 'Year'],\n",
    "                           columns = 'DataType',\n",
    "                           values = 'Qty_Val',\n",
    "                           aggfunc = 'sum')\n",
    "\n",
    "# Reset the index and show the values\n",
    "df1_forecast = df1_forecast.reset_index().rename_axis(None).rename_axis(None, axis=1)\n",
    "\n",
    "# Add new / Rename columns in line with SnOP Base Model\n",
    "df1_forecast['DataType'] = 'Forecast'\n",
    "df1_forecast = df1_forecast.rename(columns = {'Value GBP': 'GBPValue',\n",
    "                      'Actual / Forecast': 'Qty'})\n",
    "\n",
    "# Convert Months to MonthNum to get the last date of the month\n",
    "df1_forecast['MonthNum'] = df1_forecast.Month.apply(lambda x: dict((v, k) for k, v in enumerate(calendar.month_abbr))[x])\n",
    "df1_forecast['By'] = pd.to_datetime(df1_forecast.Year*10000 + \n",
    "                                    df1_forecast.MonthNum*100+1, format = \"%Y%m%d\") + pd.tseries.offsets.MonthEnd(1)\n",
    "\n",
    "#...   Get the Plan   ...\n",
    "#........................\n",
    "\n",
    "# Filter Budget (Plan)\n",
    "planfilter = (((df_cis.DataType == 'Budget') | (df_cis.DataType == 'Budget Value GBP')) &\n",
    "                (df_cis.Year.isin(years)) & \\\n",
    "                (df_cis.Month.isin(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', \\\n",
    "                                     'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])) & \\\n",
    "                (df_cis.Combi != 'TOTAL') & \\\n",
    "                (df_cis.Qty_Val != 0))\n",
    "\n",
    "df_plan = df_cis[planfilter]\n",
    "\n",
    "# Pivot to get sum of Plan, Month and Year Values\n",
    "df1_plan = df_plan.pivot_table(index = ['Combi', 'Month', 'Year'],\n",
    "                           columns = 'DataType',\n",
    "                           values = 'Qty_Val',\n",
    "                           aggfunc = 'sum').reset_index().rename_axis(None).rename_axis(None, axis=1)\n",
    "\n",
    "# Set up columns with correct names\n",
    "df1_plan['DataType'] = 'Plan'\n",
    "df1_plan = df1_plan.rename(columns = {'Budget':'Qty', 'Budget Value GBP':'GBPValue' })\n",
    "\n",
    "# Multiply the budget value with 1,000\n",
    "df1_plan.GBPValue = df1_plan.GBPValue * 1000\n",
    "\n",
    "# Convert Months to MonthNum\n",
    "df1_plan['MonthNum'] = df1_plan.Month.apply(lambda x: dict((v, k) for k, v in enumerate(calendar.month_abbr))[x])\n",
    "\n",
    "\n",
    "# Remove 2019 of Cognos from data from df1_plan, because this is coming from Jared + Darshil's FDS file\n",
    "df1_plan = df1_plan[df1_plan.Year == 2018].reset_index(drop=True)\n",
    "\n",
    "#... Get the details from Volume file ...\n",
    "#........................................\n",
    "\n",
    "# Filter the Volume file\n",
    "volumefilter = (df_volume.Month.isin(['Prod Name', 'Cust Name', 'Cust Country', \\\n",
    "                                      'Item ID', 'AM'])) & (df_volume.Year.isin(years))\n",
    "\n",
    "df1_volume = df_volume[volumefilter]\n",
    "\n",
    "# Pivot and split out items into columns from rows in 'Month' column\n",
    "df1_volume = df1_volume.pivot_table(index=['DataType', 'Combi', 'Year'], columns=['Month'], \n",
    "                         values='Qty_Val', aggfunc=max)\n",
    "\n",
    "df1_volume = df1_volume.reset_index().rename_axis(None).rename_axis(None, axis=1)\n",
    "\n",
    "# Drop DataType and make it unique on 'Combi'\n",
    "df1_volume = df1_volume.drop('DataType', axis=1)\n",
    "\n",
    "df1_volume = df1_volume.drop_duplicates('Combi')\n",
    "\n",
    "df1_volume = df1_volume.drop('Year', axis=1)\n",
    "\n",
    "#...  Prepare the Cognos file ......\n",
    "#...................................\n",
    "\n",
    "# Merge forecast and plan\n",
    "df_cognos = pd.concat([df1_forecast, df1_plan], ignore_index=True, sort=True)\n",
    "\n",
    "\n",
    "# Merge with Volume file for details\n",
    "df1_cognos = pd.merge(df_cognos, df1_volume, on='Combi')\n",
    "\n",
    "# Split Combi into its constituents\n",
    "df1_cognos = df1_cognos.assign(**df1_cognos.Combi.str.split\n",
    "                       (' - ', expand=True).rename(columns = {0:'CustomerNo',\n",
    "                                                              1:'ProductID', \n",
    "                                                              2:'Site_ID'}))\n",
    "\n",
    "# Cognos quantities are in Millions. Change this to Units by multiplying by 1,000,000\n",
    "df1_cognos['Qty'] = df1_cognos['Qty'] * 1000000\n",
    "\n",
    "# Set By to the last date of the month\n",
    "df1_cognos['By'] = (pd.to_datetime(df1_cognos.Year*10000+df1_cognos.MonthNum*100+1, format = \"%Y%m%d\") + \\\n",
    "                  pd.tseries.offsets.MonthEnd(1)).dt.date\n",
    "\n",
    "# Map to the standardized Site\n",
    "df1_cognos['Site'] = df1_cognos['Site_ID'].map(siteID_dict)\n",
    "\n",
    "#...... Set to Essentra's accounting calendar ...\n",
    "#................................................\n",
    "\n",
    "# Some quarterly months are tricky in Essentra's accounting calendar. They are quarter end, \n",
    "# but donot align with Cognos!\n",
    "change_qtr_last_day = {datetime.datetime(2018, 9, 30).date(): datetime.datetime(2018, 9, 29).date(),\n",
    "                datetime.datetime(2019, 9, 30).date(): datetime.datetime(2019, 9, 28).date(),\n",
    "\t\t\t\tdatetime.datetime(2019, 3, 31).date(): datetime.datetime(2019, 3, 30).date()}\n",
    "\n",
    "df1_cognos.By = df1_cognos.By.replace(change_qtr_last_day)\n",
    "\n",
    "df1_cognos = get_essby(df1_cognos)\n",
    "\n",
    "# Cleanup columns and add new ones\n",
    "df2_cognos = df1_cognos.drop(['Combi', 'Month', 'Year', 'MonthNum', 'Item ID', 'Site_ID'], axis=1)\n",
    "\n",
    "df2_cognos = df2_cognos.rename(columns={\"Prod Name\": \"ProductDesc\",\n",
    "                       \"Cust Name\": \"CustName\",\n",
    "                       \"Cust Country\": \"CustCountry\"})\n",
    "df2_cognos['Order'] = ''\n",
    "\n",
    "df3_cognos = df2_cognos[base_cols+['EssBy']].reset_index(drop=True)\n",
    "\n",
    "# Add Year_Priod and Period fields to Cognos\n",
    "df3_cognos['Year_Period'] = df3_cognos.EssBy.astype(str).str.slice(0,7)\n",
    "df3_cognos['Period'] = df3_cognos.Year_Period.str.slice(5,7).astype(int)\n",
    "\n",
    "#*********************************************************************\n",
    "#                            FourthShift Cleanup     \n",
    "#_____________________________________________________________________\n",
    "\n",
    "#... Cleanup the orders file ...\n",
    "#...............................\n",
    "\n",
    "df1_orders = df_orders.rename(columns = {'PromDock_InvoiceDate': 'By',\n",
    "                          'CustomerCountry': 'CustCountry',\n",
    "                          'CustomerName': 'CustName',\n",
    "                          'Order_Invoice_No': 'Order',\n",
    "                          'AccountingPeriod': 'Period'})\n",
    "\n",
    "# Select only current year's data\n",
    "df1_orders = df1_orders.loc[df1_orders.By >= start_date, :]\n",
    "\n",
    "# Remove interco databy taking out CustName containing 'ESS', except for 'ALTESS' \n",
    "# and order containing CFG from Singapore\n",
    "\n",
    "intercomask = df1_orders.CustName.str.contains('ESS', case=False) & \\\n",
    "              ~((df1_orders.CustName.str.contains('ALTESS', case=False)) | \\\n",
    "               (df1_orders.Order.str.contains('CFG', case=False)))\n",
    "\n",
    "df2_orders = df1_orders[~intercomask].reset_index(drop=True)\n",
    "\n",
    "# df2_orders = df1_orders.copy()\n",
    "\n",
    "#... ItemUM cleanup\n",
    "# Strip ItemUM of whitespace and nulls\n",
    "df2_orders.ItemUM = df2_orders.ItemUM.str.strip()\n",
    "df2_orders.loc[df2_orders.ItemUM.isnull(), \"ItemUM\"] = ''\n",
    "\n",
    "# Replace MH and KR in ItemUm to TH\n",
    "um_search = ['MH', 'KR']\n",
    "df2_orders.loc[df2_orders.ItemUM.str.contains('|'.join(um_search), case=False), 'ItemUM'] = 'TH'\n",
    "\n",
    "# Remove items with UoM = EA, but without RY, POP or VE in them. These are garbage (packaging and other data)\n",
    "ryo_pop_search = ['RY', 'POP', 'VE']\n",
    "df3_orders=df2_orders.loc[~(df2_orders.ItemUM.isin(['EA']) & \n",
    "                        ~(df2_orders.ProductID.str.contains('|'.join(ryo_pop_search), \n",
    "                                                            case=False, na=False))), :]\n",
    "\n",
    "# Remove raw material orders - with ItemUM = KG, BB and CS\n",
    "df4_orders = df3_orders[~(df3_orders.ItemUM.isin(['KG', 'BB', 'CS']))]\n",
    "\n",
    "# Remove freight entries\n",
    "df4_orders = df4_orders[df4_orders.ProductID.str.contains('FREIGHT', case=False) == False]\n",
    "\n",
    "# Remove CPTT from Dubai\n",
    "# df4_orders = df4_orders[~((df4_orders.Order.str.contains('CPTT', case=False) |\n",
    "#                           df4_orders.ItemUM.str.contains('TH', case=False)) &\n",
    "#                         (df4_orders.Site == 'AE'))]\n",
    "\n",
    "# df4_orders.loc[(df4_orders.DataType == 'Order') &\n",
    "#                (df4_orders.Period == 10) & \n",
    "#                (df4_orders.AccountingYear == 2018), 'GBPValue'].sum()\n",
    "\n",
    "#.... RYO POP handling\n",
    "#.....................\n",
    "\n",
    "# Remove duplicates and description fields in df_ryo_pop\n",
    "df_ryo_pop1 = df_ryo_pop.drop_duplicates(subset = 'Conv2TH').drop('Description', axis=1)\n",
    "\n",
    "# Derive multiple's for ProductID: RYO_POP\n",
    "df4_orders = pd.merge(df4_orders, df_ryo_pop1, left_on= 'ProductID',right_on='RYO_POP', how=\"left\")\n",
    "\n",
    "# For items that are 'RY' and 'POP', \n",
    "# ...with UoM as 'EA' and Conv2TH is a number, multiply it to Qty\n",
    "# ...with UoM as 'EA' but Conv2TH is not a number, use conversion of 0.0125 * Qty\n",
    "# ...change the UoM to 'TH'\n",
    "\n",
    "# filter for RYO and POP only\n",
    "df_ryo_pop_filter = ['RY', 'POP']\n",
    "\n",
    "c1 = df4_orders.ProductID.str.contains('|'.join(df_ryo_pop_filter), case=False)\n",
    "c2 = df4_orders.ItemUM.str.contains('EA', case=False)\n",
    "c3 = pd.to_numeric(df4_orders.Conv2TH, errors='coerce').isnull()\n",
    "\n",
    "# Multiply Conv2TH to Qty\n",
    "df4_orders.loc[c1 & c2 & ~c3, 'Qty'] = pd.to_numeric(df4_orders.Qty) * pd.to_numeric(df4_orders.Conv2TH)\n",
    "\n",
    "# Multiply Qty with 0.0125 (indicative) for null Conv2TH\n",
    "df4_orders.loc[c1 & c2 & c3, 'Qty'] = pd.to_numeric(df4_orders.Qty) * 0.0125\n",
    "\n",
    "# Change these UoM to 'TH'\n",
    "df4_orders.loc[c1 & c2, 'ItemUM'] = 'TH'\n",
    "\n",
    "# Convert all ItemUM = 'TH' to 'EA'\n",
    "df4_orders.loc[df4_orders.ItemUM == 'TH', 'Qty'] = pd.to_numeric(df4_orders.Qty) * 1000\n",
    "df4_orders.loc[df4_orders.ItemUM == 'TH', 'ItemUM'] = 'EA'\n",
    "\n",
    "# Change all orders to OpenOrder\n",
    "df4_orders.loc[df4_orders.DataType == 'Order', 'DataType'] = 'OpenOrder'\n",
    "\n",
    "# Derive EssBy for orders\n",
    "\n",
    "# df4_orders['EssBy'] = [k for x in df4_orders.By \n",
    "#                        for i, j, k in zip(df_periods.Start, df_periods.End, df_periods.EssBy) \n",
    "#                        if i <= x <= j]\n",
    "\n",
    "df4_orders = get_essby(df4_orders)\n",
    "\n",
    "#... Extract Brazil data (OpenOrders and Invoices) ...\n",
    "#.....................................................\n",
    "\n",
    "df_brazil = df_brazil.loc[df_brazil.By >= start_date, :]\n",
    "\n",
    "df_brazil = df_brazil[['DataType', 'Site', 'CustomerNo', 'CustName', 'CustCountry', 'AM', \n",
    "                 'ProductID', 'ProductDesc', 'Order', 'By', 'Qty', 'GBPValue', 'StdGBPCost', 'Period']]\n",
    "\n",
    "# Add UoM for conversion\n",
    "df_brazil['ItemUM'] = 'TH'\n",
    "\n",
    "# Extract Invoices and Copy them as Shipment\n",
    "df_brazil_ship = df_brazil.loc[df_brazil.DataType == 'OpenOrder'].reset_index()\n",
    "df_brazil_ship.loc[df_brazil_ship.DataType == 'Invoice', 'DataType'] = 'Shipment'\n",
    "\n",
    "# Add Shipment to df_brazil\n",
    "df_brazil = pd.concat([df_brazil, df_brazil_ship], axis=0, ignore_index=True, sort=True)\n",
    "\n",
    "# convert all ItemUM = 'TH' to 'EA'\n",
    "df_brazil.loc[df_brazil.ItemUM == 'TH', 'Qty'] = pd.to_numeric(df_brazil.Qty) * 1000\n",
    "\n",
    "df_brazil = get_essby(df_brazil)\n",
    "\n",
    "# Integrate orders with Brazil\n",
    "df5_orders = pd.concat([df_brazil, df4_orders], sort=True)\n",
    "\n",
    "#... Generate EssBy for df_f ..........\n",
    "#......................................\n",
    "\n",
    "df_f = pd.concat([df_f1, df_f2, df_f3, df_plan19], sort=True)\n",
    "\n",
    "# Rename 'AM Code' column to 'AM'\n",
    "df_f = df_f.rename(columns={'AM Code': 'AM'})\n",
    "\n",
    "df_f.By = pd.to_datetime(df_f.By).dt.date\n",
    "\n",
    "df_f.By = df_f.By.replace(change_qtr_last_day) # Adjusted the Septembers\n",
    "\n",
    "# Replace dates of df_f before min(df_periods.Start) to df_periods.Start\n",
    "df_f.loc[df_f.By < start_date, 'By'] = start_date\n",
    "\n",
    "# Get EssBy and limit dataframe to minimum df_periods.Start\n",
    "df_f = get_essby(df_f)\n",
    "\n",
    "#... Generate EssBy for actuals ........\n",
    "#.......................................\n",
    "\n",
    "# For less than start_date push it to the first month.\n",
    "df1_actuals = df_actuals.copy()\n",
    "df1_actuals.loc[df1_actuals.By < start_date, 'By'] = df1_actuals.By.map(get_last_day) + \\\n",
    "                                                   datetime.timedelta(days=15)\n",
    "\n",
    "# build Year_Period\n",
    "df1_actuals_period = pd.to_datetime(df1_actuals.By).dt.strftime('%Y') \\\n",
    "                     +'-'+ df1_actuals.Period.map(\"{:02}\".format)\n",
    "\n",
    "# use dict to map EssBy\n",
    "df1_actuals['EssBy'] = df1_actuals_period.map(df_essdict)\n",
    "\n",
    "# Make the Actuals dataframe\n",
    "df2_actuals = df1_actuals.copy()\n",
    "\n",
    "df2_actuals.DataType = 'Actual'\n",
    "\n",
    "#*********************************************************************\n",
    "#                            'Demand' creation     \n",
    "#_____________________________________________________________________\n",
    "\n",
    "#...... Create EstSales ..............\n",
    "#.....................................\n",
    "\n",
    "# Get Shipment + OpenOrders for current month as EstSales\n",
    "df0_estsales = df5_orders.loc[((df5_orders.DataType == 'Shipment') | \n",
    "                               (df5_orders.DataType == 'OpenOrder')) & \n",
    "                               (df5_orders.EssBy == thismonthend), :]\n",
    "\n",
    "# Make future month forecasts as EstSales\n",
    "df1_estsales = df3_cognos.loc[(df3_cognos.DataType == 'Forecast') & \n",
    "                              (df3_cognos.EssBy > thismonthend), :]\n",
    "\n",
    "# Create EstSales\n",
    "\n",
    "df_estsales = pd.concat([df0_estsales, df1_estsales], axis=0, sort=True).reset_index(drop=True)\n",
    "df_estsales.loc[:, 'DataType'] = 'EstSales'\n",
    "\n",
    "#... Consolidate cognos, f1, f2, f3, actuals, estsales ...\n",
    "#................................................\n",
    "\n",
    "base_cols1 = base_cols + ['EssBy', 'Period']\n",
    "\n",
    "df0_demand = pd.concat([df3_cognos[base_cols1], df_f[base_cols1], df1_actuals[base_cols1],\n",
    "                        df2_actuals[base_cols1], df_estsales[base_cols1]]).reset_index(drop=True)\n",
    "\n",
    "df1_demand = pd.concat([df0_demand[base_cols1], df5_orders[base_cols1]], axis=0, sort=True)\n",
    "\n",
    "# Add manufacturing site\n",
    "df1_demand['MfgSite'] = np.where((df1_demand[\"Site\"] == 'SG') & \n",
    "                                 (df1_demand.CustomerNo.str.slice(4,5) == 'T'), 'TH', df1_demand.Site)\n",
    "df1_demand['MfgSite'] = np.where((df1_demand[\"Site\"] == 'SG') & \n",
    "                                 (df1_demand.CustomerNo.str.slice(4,5) != 'T'), 'ID', df1_demand.MfgSite)\n",
    "df1_demand['MfgSite'] = np.where((df1_demand[\"Site\"] == 'BR'), 'PA', df1_demand.MfgSite)\n",
    "\n",
    "# Split SG to TH and ID. Consolidate BR into PA.\n",
    "df1_demand.loc[(df1_demand[\"Site\"] == 'SG') & (df1_demand.CustomerNo.str.slice(4,5) == 'T'), \"Site\"] = 'TH'\n",
    "df1_demand.loc[(df1_demand[\"Site\"] == 'SG') & (df1_demand.CustomerNo.str.slice(4,5) != 'T'), \"Site\"] = 'ID'\n",
    "df1_demand.loc[(df1_demand[\"Site\"] == 'BR'), \"Site\"] = 'PA'\n",
    "\n",
    "# Manufacturing site change - based on special routing \n",
    "df_mfgsite = df_mfgsitechange[['CustomerNo', 'ProductID', 'Site', 'MfgSite']]\n",
    "mfgsite_dict = df_mfgsite.set_index(['CustomerNo', 'ProductID', 'Site']).to_dict(orient='index')\n",
    "mfgsite_dict = {(k[0], k[1], k[2]): vm for k, v in mfgsite_dict.items() for km, vm in v.items()}\n",
    "\n",
    "l = [i for i in zip(df1_demand.CustomerNo, df1_demand.ProductID, df1_demand.Site)]\n",
    "\n",
    "def catch(func, handle=lambda e : e, *args, **kwargs):\n",
    "    try:\n",
    "        return func(*args, **kwargs)\n",
    "    except Exception as e:\n",
    "        return np.nan\n",
    "\n",
    "m = [catch(lambda: mfgsite_dict[m]) for m in l]\n",
    "df1_demand['MfgSite'] = m\n",
    "\n",
    "# Replace MfgSite having nan with Site\n",
    "df1_demand.loc[~df1_demand.MfgSite.notnull(), \n",
    "               'MfgSite'] = df1_demand.loc[~df1_demand.MfgSite.notnull(), 'Site']\n",
    "\n",
    "#.....   Handle no current month orders in India .....\n",
    "#.....................................................\n",
    "\n",
    "if no_india_orders:\n",
    "    \n",
    "    # Demand without EstSales in India for the current period\n",
    "    df2_demand = df1_demand.loc[~((df1_demand.DataType == 'EstSales') & \n",
    "                       (df1_demand.Site == 'IN') & \n",
    "                       (df1_demand.EssBy == thismonthend)), :]\n",
    "\n",
    "    # Demand with Forecast in India for current period (to replace EstSales)\n",
    "    df3_demand = df2_demand.loc[((df2_demand.DataType == 'Forecast') & \n",
    "                       (df2_demand.Site == 'IN') & \n",
    "                       (df2_demand.EssBy == thismonthend)), :].reset_index(drop=True)\n",
    "\n",
    "    df3_demand.loc[:, 'DataType'] = 'EstSales'\n",
    "\n",
    "    # Concatenate with remaining demand\n",
    "    df2_demand = pd.concat([df2_demand, df3_demand]).reset_index(drop=True)\n",
    "else:\n",
    "    df2_demand = df1_demand.copy()\n",
    "\n",
    "#.....   Handle no actuals for last month  ......\n",
    "#................................................\n",
    "\n",
    "last_month_EssBy = get_last_day(datetime.datetime.now().replace(day=15).date()\n",
    "                                -datetime.timedelta(days=17))\n",
    "\n",
    "no_actuals = df1_actuals.By.max() < last_month_EssBy\n",
    "\n",
    "if no_actuals:\n",
    "    df_tmp_actuals = df2_demand.loc[(df2_demand.EssBy == last_month_EssBy) & \n",
    "                   ((df2_demand.DataType == 'OpenOrder') |\n",
    "                   (df2_demand.DataType == 'Shipment')) , :].reset_index(drop=True)\n",
    "    \n",
    "    df_tmp_actuals.loc[:, 'DataType'] = 'EstSales'\n",
    "    \n",
    "    df2_demand = pd.concat([df2_demand, df_tmp_actuals]).reset_index(drop=True)\n",
    "\n",
    "#*********         Fillups and Lookups    ****************\n",
    "#______________________________________________________\n",
    "\n",
    "df3_demand = df2_demand.copy()\n",
    "\n",
    "# Fill up the TLAs\n",
    "\n",
    "df_prodref = pd.read_excel(lookups_xls, 'ProductRef', header=0, skiprows=4)\n",
    "df_prodref = df_prodref.rename(columns = {'S1': 'Seg1', \n",
    "                                            'S2': 'Seg2', 'S3': 'Seg3', 'S4': 'Seg4' })\n",
    "df_prodref = df_prodref.fillna('')       # Replace all NaN with blanks\n",
    "df3_demand['TLA'] = df3_demand.ProductID.str.slice(0,3)\n",
    "\n",
    "df3_demand = pd.merge(df3_demand, df_prodref, left_on='TLA', \n",
    "                             right_on='TLA', how='left')\n",
    "\n",
    "#.....        CustomerGrp and CustomerType Lookup      ......\n",
    "#............................................................\n",
    "df_custgrp = df_custgrp.drop_duplicates('CustName')\n",
    "\n",
    "df_custgrp1 = df_custgrp[['CustName', 'CustomerGrp', 'Region', 'CustomerType']]\n",
    "\n",
    "df3_demand = pd.merge(df3_demand, df_custgrp1, on = 'CustName', how = 'left')\n",
    "\n",
    "# Replace missing CustCountry from looked-up CustName -> CustCountry fields\n",
    "df_custcountry = df_custgrp[['CustName', 'CustCountry']].set_index('CustName')\n",
    "mask_cc = df3_demand.CustCountry.isnull()\n",
    "df3_demand.loc[mask_cc, 'CustCountry'] = df3_demand.loc[mask_cc, \n",
    "                                                    :].CustName.map(df_custcountry.CustCountry)\n",
    "\n",
    "#.....                        AM Lookup                ........\n",
    "#..............................................................\n",
    "\n",
    "df_am = df_am.drop_duplicates('AM Code')\n",
    "\n",
    "df3_demand = pd.merge(df3_demand, df_am, left_on= \"AM\", right_on='AM Code', \n",
    "         how='left').drop(['AM', 'AM Code'], \n",
    "                          axis=1).rename(columns={\"AM Name\": \"AM\"})\n",
    "\n",
    "# Replace missing AMs from looked-up CustName -> AM fields\n",
    "df_custAM = df_custgrp[['CustName', 'AM Name']].set_index('CustName')\n",
    "mask_am = df3_demand.AM.isnull() & df3_demand.CustName.notnull()\n",
    "df3_demand.loc[mask_am, 'AM'] = df3_demand.loc[mask_am, :].CustName.map(df_custAM['AM Name'])\n",
    "\n",
    "#... Identifying Game Changers ...\n",
    "#.................................\n",
    "df_gamechangers = pd.read_excel(lookups_xls, 'GameChangers', header=0, skiprows=4, usecols=[1,2])\n",
    "\n",
    "df_gc_dict = {df['Item']: df['GameC'] for df in df_gamechangers.to_dict(orient='records')}\n",
    "\n",
    "df3_demand['GameC'] = df3_demand.TLA.map(df_gc_dict)\n",
    "df3_demand.GameC = df3_demand.ProductID.map(df_gc_dict).fillna(df3_demand.GameC)\n",
    "\n",
    "# If there is any demand in month 13, replace it with 'By' period's extract\n",
    "df3_demand.loc[(df3_demand.Period == 13), 'Period'] = pd.to_datetime(df3_demand.loc[(df3_demand.Period == 13), 'By']).dt.month\n",
    "\n",
    "##*********************************************************************\n",
    "#                            'Supply' creation     \n",
    "#_____________________________________________________________________\n",
    "\n",
    "# Keep only Order, Forecast and EstSales in demand\n",
    "demand = df3_demand.loc[(df3_demand['DataType'] == 'OpenOrder') | \n",
    "                        (df3_demand['DataType'] == 'EstSales') |\n",
    "                        (df3_demand['DataType'] == 'Forecast')]\n",
    "demand = demand.reset_index(drop=True)\n",
    "\n",
    "#... Determine appropriate manufacturing site\n",
    "#............................................\n",
    "\n",
    "# For Singapore and Brazil\n",
    "demand.loc[(demand[\"Site\"] == 'SG') & (demand.CustomerNo.str.slice(4,5) == 'T'), \"Site\"] = 'TH'\n",
    "demand.loc[(demand[\"Site\"] == 'SG') & (demand.CustomerNo.str.slice(4,5) != 'T'), \"Site\"] = 'ID'\n",
    "demand.loc[(demand[\"Site\"] == 'BR'), \"Site\"] = 'PA'\n",
    "\n",
    "# For out-of-self manufacturing sites (like Dubai)\n",
    "mfg_site = df_mfgsitechange[[\"CustomerNo\", \"ProductID\", \"MfgSite\"]]\n",
    "\n",
    "mfg_site = mfg_site.sort_values(by= [\"CustomerNo\", \"ProductID\"])\n",
    "demand = demand.sort_values(by=['CustomerNo', 'ProductID'])\n",
    "\n",
    "mfg_site = mfg_site.set_index(['CustomerNo', 'ProductID'])\n",
    "demand = demand.set_index(['CustomerNo', 'ProductID'])\n",
    "demand.update(mfg_site) # Update MfgSite column\n",
    "\n",
    "mfg_site1 = df_mfgsitechange[[\"CustomerNo\", \"ProductID\", \"MfgSite\"]]\n",
    "mfg_site1.columns = [\"CustomerNo\", \"ProductID\", \"Site\"]\n",
    "mfg_site1 = mfg_site1.set_index(['CustomerNo', 'ProductID'])\n",
    "demand.update(mfg_site1) # Update Site column to be the same as MfgSite\n",
    "\n",
    "demand = demand.reset_index()\n",
    "demand = demand.drop('BRMult', axis=1)\n",
    "\n",
    "#... Integrate with BOM\n",
    "# Weed out the BOM of unnecessary rows\n",
    "bom = df_bom.loc[df_bom.ComponentItemNumber.str.match('...(\\d)') &\n",
    "        df_bom.ComponentItemNumber.str.match('..([A-Z])') &\n",
    "        (df_bom.ComponentItemNumber.str[:3] != 'CON') &\n",
    "        (df_bom.RequiredQuantity != 0) &\n",
    "        (df_bom.ComponentType.str[0] == 'N') &\n",
    "        (pd.to_datetime(df_bom.OutEffectivityDate) >= \n",
    "            datetime.datetime.now()), :].reset_index(drop=True)\n",
    "\n",
    "machine_cols = [\"Site\", \"Machine\", \"MachineID\", \"MachineType\", \n",
    "                     \"Category\", \"Size\", \"Spec1\", \"Spec2\", \"Spec3\",\n",
    "                     \"DemoCap_ph\", \"MaxCap_ph\", \n",
    "                     \"Shift\", \"OfflineFrom\", \"OfflineTo\"]\n",
    "\n",
    "# Add a column to demand and bom with 'SiteID/ProductIDmachine_cols'\n",
    "demand[\"SiteProduct\"] = demand[\"Site\"] + '/' + demand[\"ProductID\"]\n",
    "bom[\"SiteProduct\"] = bom[\"Site\"] + '/' +  bom[\"ItemNumber\"]\n",
    "\n",
    "# ...Convert Forecast to EstSales for India for demand\n",
    "#    This is to prevent the India demand shortfall to supply during the first 10 days\n",
    "\n",
    "# remove EstSales for India in this month\n",
    "demand = demand[~((demand.Site == 'IN') & \n",
    "       (demand.DataType == 'EstSales') &\n",
    "       (demand.EssBy == thismonthend))]\n",
    "\n",
    "# Fish out the current month's forecast for India and call it to be EstSales\n",
    "in_curr_estsales = demand[((demand.Site == 'IN') & \n",
    "       (demand.DataType == 'Forecast') &\n",
    "       (demand.EssBy == thismonthend))]\n",
    "\n",
    "in_curr_estsales = in_curr_estsales.assign(DataType = 'EstSales')\n",
    "\n",
    "# append India's estimated sales to demand\n",
    "demand = demand.append(in_curr_estsales).reset_index(drop=True)\n",
    "\n",
    "# remove Forecast from demand. Everything below should be based on EstSales\n",
    "demand = demand[demand.DataType != 'Forecast']\n",
    "\n",
    "####------------------------------------------------------------------------------------\n",
    "#        PREPARING BASEROD DEMAND\n",
    "####------------------------------------------------------------------------------------\n",
    "\n",
    "# Merge demand with bom to get baserod demand\n",
    "cols_to_drop = ['SiteProduct', 'ItemNumber', 'Site_y']\n",
    "baserod = pd.merge(demand, bom, how='inner', on='SiteProduct').drop(cols_to_drop, 1)   \\\n",
    "    .rename(columns={'ComponentItemNumber': 'BaseRod', 'RequiredQuantity':'BRMult', 'Site_x': 'Site'})\n",
    "\n",
    "    \n",
    "baserod.loc[(baserod.DataType == \"OpenOrder\") & \n",
    "           ((baserod.Category == \"Dual\") | (baserod.Category == \"Triple\") | \n",
    "           (baserod.Category == \"Quad\")),\n",
    "           \"DataType\"] = 'OrdrBR'\n",
    "\n",
    "baserod.loc[(baserod.DataType == \"EstSales\") & \n",
    "           ((baserod.Category == \"Dual\") | (baserod.Category == \"Triple\") | \n",
    "           (baserod.Category == \"Quad\")),\n",
    "           \"DataType\"] = 'FcstBR'\n",
    "\n",
    "baserod.loc[~((baserod.DataType == \"OrdrBR\") | (baserod.DataType == \"FcstBR\")) &\n",
    "            (baserod.DataType == \"OpenOrder\"),\n",
    "            \"DataType\"] = 'OrdrOth'\n",
    "\n",
    "baserod.loc[~((baserod.DataType == \"OrdrBR\") | (baserod.DataType == \"FcstBR\")) &\n",
    "            (baserod.DataType == \"EstSales\"),\n",
    "            \"DataType\"] = 'FcstOth'\n",
    "\n",
    "# Add baserod to ProductID\n",
    "# baserod.ProductID = baserod.BaseRod + '->' +baserod.ProductID\n",
    "baserod.rename(columns={\"ProductID\": \"Parent\", \"BaseRod\": \"ProductID\"}, inplace=True)\n",
    "\n",
    "# Change category for Dual, Triple and Quad to Mono.\n",
    "# This is to match against Mono capacity.\n",
    "\n",
    "baserod.loc[(baserod.Category == \"Dual\") | (baserod.Category == \"Triple\") | (baserod.Category == \"Quad\"),\n",
    "             \"Category\"] = \"Mono\"\n",
    "\n",
    "# # Add standalone baserrod sku to baserod demand\n",
    "mono_sku = demand.loc[((demand.DataType == \"OpenOrder\") | (demand.DataType == \"EstSales\")) &(demand.Category == \"Mono\")]\n",
    "\n",
    "# Make the DataType as FcstBR\n",
    "mono_sku = mono_sku.reset_index(drop=True)\n",
    "mono_sku.loc[(mono_sku.DataType == \"OpenOrder\"), \"DataType\"] = \"OrdrBR\"\n",
    "mono_sku.loc[(mono_sku.DataType == \"EstSales\"), \"DataType\"] = \"FcstBR\"\n",
    "\n",
    "# Add BRMult column with 1 to mono_sku\n",
    "mono_sku.insert(loc=8, column=\"BRMult\",value=1)\n",
    "\n",
    "# baserod['SiteProduct'] = ''\n",
    "# mono_sku['Parent'] = ''\n",
    "\n",
    "baserod = pd.concat((baserod, mono_sku), axis=0, ignore_index=True, sort=True)\n",
    "\n",
    "baserod.Qty = baserod.Qty * baserod.BRMult\n",
    "\n",
    "baserod = baserod[baserod.DataType.isin(['FcstBR', 'OrdrBR'])]\n",
    "\n",
    "####------------------------------------------------------------------------------------\n",
    "#        PREPARING COMBINER DEMAND\n",
    "####------------------------------------------------------------------------------------\n",
    "\n",
    "# For Combiner Order\n",
    "demand.loc[(demand.DataType == \"OpenOrder\") & \n",
    "          ((demand.Category == \"Dual\") | (demand.Category == \"Triple\") | (demand.Category == \"Quad\")),\n",
    "           \"DataType\"] = 'OrderComb'\n",
    "\n",
    "# For Combiner Forecast\n",
    "demand.loc[(demand.DataType == \"EstSales\") & \n",
    "          ((demand.Category == \"Dual\") | (demand.Category == \"Triple\") | (demand.Category == \"Quad\")),\n",
    "           \"DataType\"] = 'FcstComb'\n",
    "\n",
    "# drop SiteProduct\n",
    "demand = demand.drop(\"SiteProduct\", 1)\n",
    "\n",
    "# keep only combiner demand\n",
    "demand = demand[demand.DataType.isin([\"FcstComb\", \"OrderComb\"])]\n",
    "\n",
    "####------------------------------------------------------------------------------------\n",
    "#        PREPARE OVERALL DEMAND PLAN (BASEROD + COMBINER)\n",
    "####------------------------------------------------------------------------------------\n",
    "\n",
    "demand_plan = pd.concat([demand, baserod], sort=True)\n",
    "\n",
    "####------------------------------------------------------------------------------------\n",
    "#        SPLIT MONTHLY DEMAND TO WEEKLY BUCKETS\n",
    "####------------------------------------------------------------------------------------\n",
    "\n",
    "# ## Set the dates\n",
    "\n",
    "first_day = df_periods.Start.min()\n",
    "\n",
    "# Set today's date as the first day of the month\n",
    "if date.today().month != 1:\n",
    "    today = date(date.today().year, date.today().month-1,1)\n",
    "else:\n",
    "    today = date(date.today().year, 1, 1)\n",
    "\n",
    "# Function to get the date of the previous Weekday before the given date\n",
    "# Monday will be 0\n",
    "onDay = lambda date, day: date - datetime.timedelta(days=(day+date.weekday()+7)%7)\n",
    "\n",
    "today = onDay(today,0) # Set today to the first Monday of the Month, even if it is in previous month\n",
    "\n",
    "# Set last day based on SnOP timeframe of 24 months\n",
    "last_day = today + pd.DateOffset(months=24)\n",
    "last_day = pd.to_datetime(last_day).date()\n",
    "\n",
    "# Generate date list of Mondays - from today till the last day\n",
    "mondays = pd.Series(pd.date_range(today, last_day, freq='W-Mon'))\n",
    "\n",
    "weeks = pd.DataFrame({'Week':mondays})\n",
    "\n",
    "# Remove dates greater than first_day and less than last_day from demand_plan\n",
    "demand_plan = demand_plan[(demand_plan.By > first_day) & (demand_plan.By < last_day )]\n",
    "\n",
    "####------------------------------------------------------------------------------------\n",
    "#        GENERATE CAPACITIES\n",
    "####------------------------------------------------------------------------------------\n",
    "\n",
    "machine = df_machine[machine_cols]\n",
    "\n",
    "# Strip all texts in machine\n",
    "machine_obj = machine.select_dtypes(['object'])\n",
    "machine[machine_obj.columns] = machine_obj.apply(lambda x: x.str.strip())\n",
    "\n",
    "# Generate WeekFrom and WeekTo records for each date in the DataFrame\n",
    "machine = machine.assign(key=0)\n",
    "\n",
    "ser = pd.DataFrame({'WeekFrom': mondays, 'key':[0] * len(mondays)}) # generate dataframe series with key\n",
    "\n",
    "machine = pd.merge(machine, ser, on = 'key').drop('key', axis = 1)\n",
    "\n",
    "machine['WeekTo'] = machine['WeekFrom'].apply(lambda x: x + timedelta(days=7))\n",
    "\n",
    "# Convert all mondays to datetime for comparison\n",
    "machine['WeekFrom'] = pd.to_datetime(machine['WeekFrom'], errors='coerce')\n",
    "machine['WeekTo'] = pd.to_datetime(machine['WeekTo'], errors='coerce')\n",
    "machine['OfflineFrom'] = pd.to_datetime(machine['OfflineFrom'], errors='coerce')\n",
    "machine['OfflineTo'] = pd.to_datetime(machine['OfflineTo'], errors='coerce')\n",
    "\n",
    "## Extract number of days per week from 'Shift' column.\n",
    "## Subtract holidays from 'Shift' column, if the holiday:\n",
    "##   - it falls within 'WeekFrom' and 'WeekTo', \n",
    "##  - and is not within 'OfflineFrom' and 'OfflineTo' \n",
    "\n",
    "# Get the days from 'Shift' and convert it to a numeric\n",
    "machine['ShiftDays'] = pd.to_numeric(machine['Shift'].str[3:])\n",
    "\n",
    "# Get the hours from 'Shift' and convert it to a numeric\n",
    "machine['ShiftHours'] = pd.to_numeric(machine['Shift'].str.slice(0,2))\n",
    "\n",
    "# Get the holidays\n",
    "df_holidays.dropna(how='all', inplace=True)\n",
    "holiday = df_holidays\n",
    "\n",
    "# Convert holiday array's values to datetime for comparison\n",
    "holiday['Holiday'] = pd.to_datetime(holiday['Holiday'])\n",
    "\n",
    "# Reduce ShiftDays by number of holidays in each site for each machine line.\n",
    "machine.ShiftDays -= ((holiday.Holiday[:, None] >= machine.WeekFrom.values)\n",
    "                   & (holiday.Holiday[:, None] <= machine.WeekTo.values) \n",
    "                   & (holiday.SiteID[:, None] == machine.Site.values)).sum(axis=0)\n",
    "\n",
    "## Condition table for Online Days:\n",
    "# Initialize Days column\n",
    "machine['Days'] = machine['ShiftDays']\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "#                                  ---OfflineFrom---OfflineTo---\n",
    "#         WeekFrom WeekTo----------|                            |-----WeekFrom WeekTo\n",
    "\n",
    "# ........There is nothing to do here as the default for these is ShiftDays.........\n",
    "# machine.loc[(((machine.WeekFrom <= machine.OfflineFrom) & (machine.WeekTo <= machine.OfflineFrom)) | \n",
    "#             ((machine.WeekFrom >= machine.OfflineTo) & (machine.WeekTo >= machine.OfflineTo))), :]\n",
    "# ....................................................................................................\n",
    "\n",
    "# ..........................     First condition       ...............................................\n",
    "#           OfflineFrom|-----------------------------------------------------|OfflineTo\n",
    "#                             WeekFrom | -------------------------|  Week To\n",
    "\n",
    "# Set mask1 to locate the condition\n",
    "mask1 = (machine.OfflineFrom <= machine.WeekFrom) & (machine.OfflineTo >= machine.WeekTo)\n",
    "machine.loc[mask1, \"Days\"] = 0\n",
    "\n",
    "# .......................        Second condition      .......................     \n",
    "#                             ---OfflineFrom-----------------OfflineTo---\n",
    "#         WeekFrom -----------|                WeekTo\n",
    "# ............................................................................\n",
    "\n",
    "# Set mask2 to locate the condition\n",
    "mask2 = ((machine.WeekFrom <= machine.OfflineFrom) & (machine.WeekTo <= machine.OfflineTo) & \n",
    "         (machine.WeekTo >= machine.OfflineFrom))\n",
    "\n",
    "# Compute the days for the mask2 based on the condition.        \n",
    "dayseries = machine.loc[mask2, \"ShiftDays\"] - \\\n",
    "           (machine.loc[mask2, \"WeekTo\"] - machine.loc[mask2, \"OfflineFrom\"]).dt.days\n",
    "\n",
    "# Set the Days and remove any negative days with max function\n",
    "machine.loc[mask2, \"Days\"] = np.maximum(dayseries, 0)\n",
    "\n",
    "# .......................       Third condition   ................................     \n",
    "#                             ---OfflineFrom-------------OfflineTo---|\n",
    "#                                          WeekFrom |-------------------WeekTo-----|\n",
    "# ................................................................................\n",
    "\n",
    "# Set mask3 to locate the condition\n",
    "mask3 = (machine.OfflineFrom < machine.WeekFrom) & \\\n",
    "        (machine.WeekFrom < machine.OfflineTo) & \\\n",
    "        (machine.OfflineTo > machine.WeekTo)\n",
    "     \n",
    "# Compute the days for the mask3 based on the condition\n",
    "dayseries3 = (machine.loc[mask3, \"WeekTo\"] - machine.loc[mask3, \"OfflineTo\"]).dt.days\n",
    "\n",
    "# Set the Days to minumum of ShiftDays and dayseries3\n",
    "machine.loc[mask3, \"Days\"] = np.minimum(machine.loc[mask3, \"ShiftDays\"], dayseries3)\n",
    "\n",
    "# Remove negative Days from machines (could happen if entire week was a holiday)\n",
    "machine.loc[machine.Days < 0, \"Days\"] = 0\n",
    "\n",
    "# Determine Demonstrated machine and Maximum machine\n",
    "machine['DemoCap'] = machine['DemoCap_ph'] * machine['ShiftHours'] * machine['Days'] * 1000\n",
    "\n",
    "machine['MaxCap'] = machine['MaxCap_ph'] * 24 * 7 * 1000\n",
    "\n",
    "# Rename 'WeekTo' to 'By' and 'DemoCap' to 'Qty'\n",
    "machine = machine.rename(columns={'WeekTo': 'By', 'DemoCap': 'Qty', 'MachineType': 'DataType'})\n",
    "\n",
    "####------------------------------------------------------------------------------------\n",
    "#        CREATE THE SUPPLY PLAN\n",
    "####------------------------------------------------------------------------------------\n",
    "\n",
    "#... Prepare supply_plan from demand_plan\n",
    "supply_plan = demand_plan.copy()\n",
    "\n",
    "# Remove out-of-range period data\n",
    "supply_plan = supply_plan[~(supply_plan.By > df_periods.End.max())].reset_index(drop=True)\n",
    "\n",
    "# Generate EssBy for Essentra calendar\n",
    "supply_plan['EssBy'] = [k for x in supply_plan.By\n",
    "                       for i, j, k in zip(df_periods.Start, df_periods.End, df_periods.EssBy) \n",
    "                       if i <= x <= j]\n",
    "\n",
    "# Add Year_Period and Period to the supply plan\n",
    "supply_plan['Year_Period'] = supply_plan.EssBy.astype(str).str.slice(0,7)\n",
    "supply_plan['Period'] = supply_plan.Year_Period.str.slice(5,7).astype(int)\n",
    "\n",
    "# Stage the machine_map for supply\n",
    "machine_map_cols = ['MachineID', 'Site', 'Year_Period', 'TLA', 'ProductID']\n",
    "machine_map = df_machinemap[machine_map_cols].drop_duplicates()\n",
    "\n",
    "# Get the MachineID column in supply_plan for TLA\n",
    "m = machine_map[['Site', 'Year_Period', 'TLA', 'MachineID']]\n",
    "supply_plan = supply_plan.merge(m.drop_duplicates(['Site', 'Year_Period', 'TLA']), how='left')\n",
    "\n",
    "#... Update the MachineID column in supply_plan for ProductID\n",
    "s = supply_plan.set_index(['Site', 'Year_Period', 'ProductID'])\n",
    "mp = machine_map[['Site', 'Year_Period', 'ProductID', 'MachineID']].drop_duplicates(['Site', 'Year_Period', 'ProductID']).set_index(['Site', 'Year_Period', 'ProductID'])\n",
    "\n",
    "s.update(mp, overwrite=False)\n",
    "\n",
    "supply_plan = s.reset_index()\n",
    "\n",
    "# merge supply_plan with compacted machine, to get additional machine fields\n",
    "cm = df_machine[['Site', 'MachineID', 'Machine', 'MaxCap_ph']].drop_duplicates()\n",
    "cm = cm.assign(MaxCap= cm.MaxCap_ph*7*24*1000).drop('MaxCap_ph', axis=1) # computed MaxCap\n",
    "\n",
    "supply_plan = supply_plan.merge(cm, how='left')\n",
    "\n",
    "#... Prepare the machine data\n",
    "# Remove out-of-range period data\n",
    "machine.By = pd.to_datetime(machine.By).dt.date # convert to date\n",
    "machine = machine[~(machine.By > df_periods.End.max())].reset_index(drop=True)\n",
    "\n",
    "machine['EssBy'] = [k for x in machine.By\n",
    "                       for i, j, k in zip(df_periods.Start, df_periods.End, df_periods.EssBy) \n",
    "                       if i <= x <= j]\n",
    "\n",
    "# Add Year_Period and Period to the machines\n",
    "machine['Year_Period'] = machine.EssBy.astype(str).str.slice(0,7)\n",
    "machine['Period'] = machine.Year_Period.str.slice(5,7).astype(int)\n",
    "\n",
    "# merge the machines with machinemap to get TLA and ProductID of the machines\n",
    "machine = machine.merge(machine_map.drop_duplicates(['MachineID', 'Site', 'Year_Period']), on=['MachineID', 'Site', 'Year_Period'], how='left')\n",
    "\n",
    "# Make a clean product reference table to update machine for deltails through TLA\n",
    "df_prodref1 = df_prodref[['TLA', 'Seg1', 'Seg2', 'Seg3', 'Seg4', 'Feature', 'SubFeature']].drop_duplicates()\n",
    "machine = pd.merge(machine, df_prodref1, on='TLA', how='left' )\n",
    "\n",
    "# concatenate supply_plan and machine to make supply\n",
    "supply = pd.concat((supply_plan, machine), axis=0, ignore_index=True, sort=True)\n",
    "\n",
    "# For ProductID with Parent, replace the TLA to be of ProductID and not of Parent\n",
    "supply.loc[supply.Parent.notnull(), 'TLA'] = supply.ProductID.str.slice(0,3)\n",
    "\n",
    "# remove 'Other' category from Supply. These are all e-Cigs\n",
    "supply = supply[supply.DataType != 'Other']\n",
    "\n",
    "####------------------------------------------------------------------------------------\n",
    "#        HANDLING EXCEPTIONS\n",
    "####------------------------------------------------------------------------------------\n",
    "\n",
    "executed = [exec(i) for i in df_exceptions.Logic]\n",
    "\n",
    "#**********************************************************************\n",
    "#                       Costing distribution\n",
    "#_______________________________________________________________________\n",
    "\n",
    "#......  Create UnitCost dictionaries ............\n",
    "#.................................................\n",
    "\n",
    "def get_avg_unit_cost(df, u):\n",
    "    '''Args: \n",
    "         df as pandas.DataFrame, u as unitcost column header\n",
    "       Returns:\n",
    "         dictionary of Site+ProductID and Average Unit Cost'''\n",
    "    df1 = df[['Site', 'ProductID', u]].dropna(subset=[u]).reset_index(drop=True)\n",
    "    df1['Product9'] = df1.Site + df1.ProductID.str.slice(0,9)\n",
    "    df1 = df1[['Product9', u]].set_index('Product9') # 9 digit ProductID\n",
    "    df1_dict = df1.groupby('Product9').mean().to_dict(orient='index')\n",
    "    thedict = {k: v1 for k, v in df1_dict.items() for k1, v1 in v.items()}\n",
    "    return thedict\n",
    "\n",
    "# Get UnitCost from Actuals\n",
    "df_actuals['UnitCost'] = df_actuals.StdGBPCost/df_actuals.Qty\n",
    "df_actuals['UnitLabourCost'] = df_actuals.Labour/df_actuals.Qty\n",
    "df_actuals['UnitMaterialCost'] = df_actuals.Material/df_actuals.Qty\n",
    "df_actuals['UnitFixedOH'] = df_actuals.FixedOH/df_actuals.Qty\n",
    "df_actuals['UnitVarOH'] = df_actuals.VarOH/df_actuals.Qty\n",
    "\n",
    "# df_actuals['UnitCost'] = df_actuals.StdGBPCost/df_actuals.Qty * np.where(df_actuals.GBPValue < 0, -1, 1)\n",
    "# df_actuals['UnitLabourCost'] = df_actuals.Labour/df_actuals.Qty * np.where(df_actuals.GBPValue < 0, -1, 1)\n",
    "# df_actuals['UnitMaterialCost'] = df_actuals.Material/df_actuals.Qty * np.where(df_actuals.GBPValue < 0, -1, 1)\n",
    "# df_actuals['UnitFixedOH'] = df_actuals.FixedOH/df_actuals.Qty * np.where(df_actuals.GBPValue < 0, -1, 1)\n",
    "# df_actuals['UnitVarOH'] = df_actuals.VarOH/df_actuals.Qty * np.where(df_actuals.GBPValue < 0, -1, 1)\n",
    "\n",
    "avg_actual_unit_cost_dict = get_avg_unit_cost(df_actuals, 'UnitCost')\n",
    "avg_actual_unit_labour_cost_dict = get_avg_unit_cost(df_actuals, 'UnitLabourCost')\n",
    "avg_actual_unit_material_cost_dict = get_avg_unit_cost(df_actuals, 'UnitMaterialCost')\n",
    "avg_actual_unit_fixedoh_cost_dict = get_avg_unit_cost(df_actuals, 'UnitFixedOH')\n",
    "avg_actual_unit_varoh_cost_dict = get_avg_unit_cost(df_actuals, 'UnitVarOH')\n",
    "\n",
    "# Get UnitCost from Orders\n",
    "df5_orders['UnitCost'] = df5_orders.StdGBPCost/df5_orders.Qty\n",
    "df5_orders['UnitLabourCost'] = df5_orders.Labour/df5_orders.Qty\n",
    "df5_orders['UnitMaterialCost'] = df5_orders.Material/df5_orders.Qty\n",
    "df5_orders['UnitFixedOH'] = df5_orders.FixedOH/df5_orders.Qty\n",
    "df5_orders['UnitVarOH'] = df5_orders.VarOH/df5_orders.Qty\n",
    "\n",
    "# df5_orders['UnitCost'] = df5_orders.StdGBPCost/df5_orders.Qty * np.where(df5_orders.GBPValue < 0, -1, 1)\n",
    "# df5_orders['UnitLabourCost'] = df5_orders.Labour/df5_orders.Qty * np.where(df5_orders.GBPValue < 0, -1, 1)\n",
    "# df5_orders['UnitMaterialCost'] = df5_orders.Material/df5_orders.Qty * np.where(df5_orders.GBPValue < 0, -1, 1)\n",
    "# df5_orders['UnitFixedOH'] = df5_orders.FixedOH/df5_orders.Qty * np.where(df5_orders.GBPValue < 0, -1, 1)\n",
    "# df5_orders['UnitVarOH'] = df5_orders.VarOH/df5_orders.Qty * np.where(df5_orders.GBPValue < 0, -1, 1)\n",
    "\n",
    "avg_order_unit_cost_dict = get_avg_unit_cost(df5_orders, 'UnitCost')\n",
    "avg_order_unit_labour_cost_dict = get_avg_unit_cost(df5_orders, 'UnitLabourCost')\n",
    "avg_order_unit_material_cost_dict = get_avg_unit_cost(df5_orders, 'UnitMaterialCost')\n",
    "avg_order_unit_fixedoh_cost_dict = get_avg_unit_cost(df5_orders, 'UnitFixedOH')\n",
    "avg_order_unit_varoh_cost_dict = get_avg_unit_cost(df5_orders, 'UnitVarOH')\n",
    "\n",
    "# Merge the Orders dictionary with Actuals (stackoverflow.com/a/26853961/7978112)\n",
    "x = avg_order_unit_cost_dict\n",
    "y = avg_actual_unit_cost_dict\n",
    "\n",
    "def merge_two_dicts(x, y):\n",
    "    '''Merges two dictionaries\n",
    "    Args:\n",
    "       (x, y) as dict objects\n",
    "    Returns:\n",
    "       y replacing values in x as a dict'''\n",
    "    z = x.copy() # start with x's keys and values\n",
    "    z.update(y)  # modifies z with y's keys and values & returns None\n",
    "    return z\n",
    "\n",
    "unitcost_dict = merge_two_dicts(avg_order_unit_cost_dict, avg_actual_unit_cost_dict)\n",
    "labour_dict = merge_two_dicts(avg_order_unit_labour_cost_dict, avg_actual_unit_labour_cost_dict)\n",
    "material_dict = merge_two_dicts(avg_order_unit_material_cost_dict, avg_actual_unit_material_cost_dict)\n",
    "fixedoh_dict = merge_two_dicts(avg_order_unit_fixedoh_cost_dict, avg_actual_unit_fixedoh_cost_dict)\n",
    "varoh_dict = merge_two_dicts(avg_order_unit_varoh_cost_dict, avg_actual_unit_varoh_cost_dict)\n",
    "\n",
    "# Make UnitPrice and Margin columns and replace infinities with nan\n",
    "df5_orders['UnitPrice'] = df5_orders.GBPValue / df5_orders.Qty\n",
    "df5_orders ['StdMargin'] = (df5_orders.UnitPrice - df5_orders.UnitCost)/df5_orders.UnitPrice\n",
    "df5_orders = df5_orders.replace([np.inf, -np.inf], np.nan)  # Replace infinities with nan\n",
    "\n",
    "# Make a Site+/TLA unitcost dictionaries\n",
    "a = pd.DataFrame.from_dict({k[0:5]: v for k, v in unitcost_dict.items()}, orient='index', columns=['UnitCost'])\n",
    "df_sitetlacost = a.reset_index().groupby('index').mean().reset_index()\n",
    "sitetlacost_dict = df_sitetlacost.to_dict(orient='index')\n",
    "sitetlacost_dict = {v['index']: v['UnitCost'] for k, v in sitetlacost_dict.items()}\n",
    "\n",
    "margin_fields = 'UnitCost,UnitLabourCost,UnitMaterialCost,UnitFixedOH,UnitVarOH,UnitPrice,StdMargin'.split(',')\n",
    "\n",
    "def get_unit_cost(df):\n",
    "    '''Extends dataframe to include unit cost columns\n",
    "    Arg: df as DataFrame\n",
    "    Returns: extended df'''\n",
    "    siteproduct = df.Site+df.ProductID.str.slice(0,9)\n",
    "\n",
    "    # Prepare the demand columns\n",
    "    df['UnitCost'] = siteproduct.map(unitcost_dict)\n",
    "    df['UnitLabourCost'] = siteproduct.map(labour_dict)\n",
    "    df['UnitMaterialCost'] = siteproduct.map(material_dict)\n",
    "    df['UnitFixedOH'] = siteproduct.map(fixedoh_dict)\n",
    "    df['UnitVarOH'] = siteproduct.map(varoh_dict)\n",
    "    \n",
    "    sitetla = df.Site+df.TLA\n",
    "    df['UnitCost'] = sitetla.map(sitetlacost_dict)\n",
    "    df.loc[df.UnitCost.isnull(), 'UnitCost'] = sitetla.map(sitetlacost_dict)\n",
    "\n",
    "#     df_wo_UnitCost = df[pd.to_numeric(df.UnitCost, errors='coerce').isnull()]\n",
    "\n",
    "    # Make Qty 1 for removing na errors\n",
    "#     df.loc[(df.DataType == 'EstSales') & (df.Qty == 0.0), 'Qty'] = 1\n",
    "    df.loc[(df.Qty == 0.0), 'Qty'] = 1\n",
    "\n",
    "    # Make UnitPrice, StdMargin, StdGBPCost and Period columns\n",
    "    df['UnitPrice'] = df.GBPValue / df.Qty\n",
    "    df['StdMargin'] = (df.UnitPrice - df.UnitCost).div(df.UnitPrice.where(df.UnitPrice !=0, np.nan))\n",
    "    df['StdGBPCost'] = df.UnitCost * df.Qty\n",
    "    df['Year_Period'] = df.EssBy.map({v:k for k, v in df_essdict.items()})\n",
    "\n",
    "    # .... Handle margin outliers  ......\n",
    "\n",
    "    # Remove costs for ProductIDs having SAM in it.\n",
    "    df.loc[df.ProductID.str.upper().str.contains('SAM', na=False), \n",
    "                 margin_fields] = pd.np.nan\n",
    "\n",
    "    if remove_outliers:\n",
    "        mask = (df.StdMargin < negative_outlier) | (df.StdMargin > positive_outlier)\n",
    "        df.loc[mask, margin_fields] = df.loc[mask, margin_fields].assign(StdMargin=np.nan)\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)  # Replace infinities with nan\n",
    "\n",
    "    return(df)\n",
    "\n",
    "# Get unit costs for demand and supply\n",
    "df_demand = get_unit_cost(df3_demand)\n",
    "\n",
    "df_supply = get_unit_cost(supply)\n",
    "\n",
    "####      Output csvs to file    #####\n",
    "#________________________________\n",
    "\n",
    "df_demand_cols = 'DataType,Site,CustomerNo,CustName,CustCountry,AM,ProductID,ProductDesc,\\\n",
    "Order,By,Qty,GBPValue,StdGBPCost,CustomerGrp,CustomerType,Region,Category,SubCat,\\\n",
    "Feature,SubFeature,Size,Period,MfgSite,TLA,GameC'.split(',')\n",
    "\n",
    "df_demand.to_csv(localpath+'\\_c4s.csv', columns = df_demand_cols, index=False)\n",
    "\n",
    "df_supply_cols_final = [\"DataType\",\"Site\",\"CustomerNo\",\"CustName\",\"CustCountry\",\"AM\",\n",
    "                     \"ProductID\",\"ProductDesc\",\"Parent\",\"Order\",\"By\",\"Qty\",\"GBPValue\",\n",
    "                     \"StdGBPCost\",\"CustomerGrp\",\"CustomerType\",\"Region\",\"Size\",\"Category\",\n",
    "                     \"MachineID\",\"Machine\",\"MaxCap\",\"Seg1\",\"Seg2\",\"Seg3\",\"Seg4\",\n",
    "                     \"Spec1\",\"Spec2\",\"Spec3\",\"Feature\",\"Year_Period\",\"TLA\"]\n",
    "\n",
    "df_supply.to_csv(path_or_buf = localpath+'\\_supply.csv', columns = df_supply_cols_final, index  = False)\n",
    "\n",
    "# Extract data for margin analysis\n",
    "df_margin = df_demand[~df_demand.StdGBPCost.isnull() & \n",
    "                      (df_demand.DataType == 'EstSales') & \n",
    "                      (df_demand.TLA.isin(list(df_prodref1.TLA)))].reset_index(drop=True)\n",
    "\n",
    "# df_margin.to_csv(path_or_buf = localpath+'\\_margin.csv', columns = df_demand_cols, index  = False)\n",
    "\n",
    "\n",
    "# For margin details extraction\n",
    "margin_cols = df_demand_cols + ['UnitLabourCost', 'UnitMaterialCost', 'UnitFixedOH', 'UnitVarOH']\n",
    "\n",
    "df_mgn = df_margin[margin_cols]\n",
    "df_mgn = df_mgn.assign(Labour = df_mgn.UnitLabourCost*df_mgn.Qty)\n",
    "df_mgn = df_mgn.assign(Material = df_mgn.UnitMaterialCost*df_mgn.Qty)\n",
    "df_mgn = df_mgn.assign(FixedOH = df_mgn.UnitFixedOH*df_mgn.Qty)\n",
    "df_mgn = df_mgn.assign(VarOH = df_mgn.UnitVarOH*df_mgn.Qty)\n",
    "\n",
    "mgn_cols = df_demand_cols + ['Labour', 'Material', 'FixedOH', 'VarOH']\n",
    "# df_mgn.to_csv(path_or_buf = localpath+'\\_margin_details.csv', columns = mgn_cols, index  = False)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print(f\"Successfully completed the program in {(end-start)/60} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#...Check for India's EstSales for current period\n",
    "df_demand[(df_demand.Site == 'IN') & \n",
    "          (df_demand.EssBy == thismonthend) &\n",
    "          (df_demand.DataType == 'EstSales')].GBPValue.sum()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#...To be run only if India's EstSales for current period is  empty / very small\n",
    "#_______________________________________________________________________________\n",
    "\n",
    "# Remove this month's EstSales\n",
    "df_x = df_demand[~((df_demand.Site == 'IN') &\n",
    "          (pd.to_datetime(df_demand.By).dt.year == 2019) &\n",
    "          (df_demand.Period == datetime.datetime.now().month) &\n",
    "          (df_demand.DataType == 'EstSales'))].reset_index(drop=True)\n",
    "\n",
    "# Get India's Forecast for this month and make it into EstSales\n",
    "df_xf = df_x[(df_x.Site == 'IN') & (df_x.EssBy == thismonthend) & (df_x.DataType == 'Forecast')].reset_index(drop=True)\n",
    "\n",
    "df_xf.loc[:, 'DataType'] = 'EstSales'\n",
    "\n",
    "df_x1 = pd.concat([df_x, df_xf], axis=0).reset_index(drop=True)\n",
    "df_x1.to_csv(localpath+'\\_c4s.csv', columns = df_demand_cols, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5_orders[(df5_orders.Site == 'IN') & (df5_orders.EssBy == datetime.date(2019, 3, 31))].GBPValue.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thismonthend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
